{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0396e7dc-7dfe-4284-a8b0-154608289f63",
   "metadata": {},
   "source": [
    "# Notebook #1 for Generation of a synthetic audio\n",
    "\n",
    "This notebook is designed to generate synthetic audio recordings based on voices from a reference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed6ef1-0850-4d8e-ada7-dedaf40af677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from TTS.api import TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef495e-dd1c-4c79-9bd2-c9ec8be864b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if not torch.cuda.is_available():\n",
    "    print('Warning: CUDA is not available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8debff-20eb-4afb-9b3d-6146247537a9",
   "metadata": {},
   "source": [
    "## 1. Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38dc29e-43b5-4322-9c38-e910f3fa2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_data_path = Path('./data/google_speech_command_v002')\n",
    "output_data_path = Path('./data/Synt-RuSC/raw/')\n",
    "\n",
    "# check ref path\n",
    "assert ref_data_path.is_dir(), 'path is not exist, check \"ref_data_path\"'\n",
    "\n",
    "# create output path\n",
    "output_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Words Lists for Target commands generation\n",
    "target_commands = [\n",
    "    'yes', 'no', 'up', 'down', 'left', \n",
    "    'right', 'on', 'off', 'stop', 'go',\n",
    "    'zero', 'one', 'two', 'three', 'four', \n",
    "    'five', 'six', 'seven', 'eight', 'nine',\n",
    "    'backward', 'forward', 'follow', 'learn', 'visual',\n",
    "]\n",
    "target_commands_ru = [\n",
    "    'да', 'нет', 'вверх', 'вниз', 'налево', \n",
    "    'направо', 'включи', 'выключи', 'стоп', 'иди',\n",
    "    'ноль', 'один', 'два', 'три', 'четыре', \n",
    "    'пять', 'шесть', 'семь', 'восемь', 'девять', \n",
    "    'назад', 'вперед', 'следуй', 'изучай', 'наблюдай',\n",
    "]\n",
    "\n",
    "# Words Lists for Consonant words generation\n",
    "non_commands_ru = [\n",
    "    'создай', 'зарыдай', 'сверх', 'разлад', 'вред', \n",
    "    'гибнет', 'гвозди', 'ржавее', 'исключи', 'девиз', \n",
    "    'беда', 'новее', 'стучи', 'сдуй', \n",
    "]\n",
    "non_commands_ref = [\n",
    "    'learn', 'visual', 'up', 'backward', 'forward', \n",
    "    'no', 'go', 'right', 'off', 'down',\n",
    "    'yes', 'left', 'on', 'follow', \n",
    "]\n",
    "non_commands_en = [\n",
    "    'create', 'cry', 'over',  'discord', 'harm', \n",
    "    'dies', 'nails', 'rustier', 'exclude', 'motto',\n",
    "    'grief', 'newer', 'knock', 'blow_off',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a38f6c4-fc69-4053-b353-d745bb3062a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nemo manifests to get file lists.\n",
    "\n",
    "with open(ref_data_path / 'test_manifest.json', 'r') as f:\n",
    "    testing_list = [json.loads(line)['audio_filepath'] \n",
    "                    for line in f if line.strip()!='']\n",
    "    testing_list = [line.replace('./google_speech_recognition_v2', \n",
    "                                 str(ref_data_path)) \n",
    "                    for line in testing_list]\n",
    "\n",
    "with open(ref_data_path / 'validation_manifest.json', 'r') as f:\n",
    "    validation_list = [json.loads(line)['audio_filepath'] \n",
    "                       for line in f if line.strip()!='']\n",
    "    validation_list = [line.replace('./google_speech_recognition_v2', \n",
    "                                    str(ref_data_path)) \n",
    "                       for line in validation_list]\n",
    "\n",
    "with open(ref_data_path / 'train_manifest.json', 'r') as f:\n",
    "    training_list = [json.loads(line)['audio_filepath'] \n",
    "                     for line in f if line.strip()!='']\n",
    "    training_list = [line.replace('./google_speech_recognition_v2', \n",
    "                                  str(ref_data_path)) \n",
    "                     for line in training_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb1414-0e93-496f-965a-f1eb358a98f3",
   "metadata": {},
   "source": [
    "## 2. Generation synthetic audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab6e16-6fd0-432c-be96-d183b07e6130",
   "metadata": {},
   "source": [
    "Target commands generation:\n",
    "- 15 words with the basic commands: 'Yes', 'No', 'Up', 'Down', 'Left',\n",
    "'Right', 'On', 'Off', 'Stop', 'Go', 'Backward', 'Forward', 'Follow',\n",
    "'Learn' and 'Visual';\n",
    "- 10 words for digits: from 'Zero' to 'Nine'\n",
    "\n",
    "Consonant words generation:\n",
    "- 14 Non-command, consonant words: создай ('create'), зарыдай ('cry'), сверх ('over'), разлад ('discord'), вред ('harm'), гибнет ('dies'), гвозди ('nails'), ржавее ('rustier'), исключи ('exclude'), девиз ('motto'), беда ('grief'), новее ('newer'), стучи ('knock'), сдуй ('blow off')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29e2b5-8d60-4f3d-babf-bc2247edd889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init TTS model\n",
    "\n",
    "# For Generation of a synthetic audio we are using xtts-v2 model from \n",
    "# https://github.com/coqui-ai/TTS  or  https://huggingface.co/coqui/XTTS-v2\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", \n",
    "          gpu=(True if device=='cuda' else False)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29849cbd-f7f0-40d7-84b6-fed318d08734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time for log\n",
    "process_start = str(datetime.now()).split('.')[0].replace(':', '-')\n",
    "\n",
    "skipped_files = []\n",
    "for subset, subset_files_list in zip(['train', 'test', 'validation'], \n",
    "                                     [training_list, testing_list, validation_list]):\n",
    "    print('subset', subset)\n",
    "    if not os.path.exists(output_data_path / subset):\n",
    "        os.mkdir(output_data_path / subset)\n",
    "    \n",
    "    \n",
    "    # Target commands generation\n",
    "    print('Target commands generation')\n",
    "    for command_ind, command_name in enumerate(target_commands):\n",
    "        command_ru_name = target_commands_ru[command_ind]\n",
    "        print('command_name', command_name, command_ru_name)\n",
    "        \n",
    "        if not os.path.exists(output_data_path / subset / command_name):\n",
    "            os.mkdir(output_data_path / subset / command_name)\n",
    "        \n",
    "        for fPath in subset_files_list:\n",
    "            fpath_cmd, fName = fPath.split('/')[-2:]\n",
    "            \n",
    "            if (fpath_cmd != command_name) or \n",
    "               (fName in os.listdir(output_data_path / subset / command_name)):\n",
    "                    continue\n",
    "            \n",
    "            # generation\n",
    "            try:\n",
    "                tts.tts_to_file(\n",
    "                    text=command_ru_name, \n",
    "                    speaker_wav=fPath, \n",
    "                    language='ru', \n",
    "                    file_path=output_data_path / subset / command_name / fName\n",
    "                )\n",
    "            except:\n",
    "                skipped_files.append(fPath)\n",
    "                with open(f'skipped_files_{process_start}.json', 'w') as f:\n",
    "                    json.dump(skipped_files, f)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Consonant words generation\n",
    "    print('Consonant words generation')\n",
    "    for word_ind, word_name in enumerate(non_commands_ru):\n",
    "        ref_word_name = non_commands_ref[word_ind]\n",
    "        en_word_name = non_commands_en[word_ind]\n",
    "        \n",
    "        print('word_name', word_name, ref_word_name, en_word_name)\n",
    "        \n",
    "        if not os.path.exists(output_data_path / subset / en_word_name):\n",
    "            os.mkdir(output_data_path / subset / en_word_name)\n",
    "        \n",
    "        for fPath in subset_files_list:\n",
    "            fpath_cmd, fName = fPath.split('/')[-2:]\n",
    "            if (fpath_cmd != ref_word_name) or\n",
    "               (subset=='train' and 'nohash_0' not in fName) or\n",
    "               (fName in os.listdir(output_data_path / subset / en_word_name)):\n",
    "                    continue\n",
    "            \n",
    "            try:\n",
    "                tts.tts_to_file(\n",
    "                    text=word_name, \n",
    "                    speaker_wav=fPath, \n",
    "                    language='ru', \n",
    "                    file_path=output_data_path / subset / en_word_name / fName\n",
    "                )\n",
    "            except:\n",
    "                skipped_files.append(fPath+f'({en_word_name})')\n",
    "                with open(f'skipped_files_{process_start}.json', 'w') as f:\n",
    "                    json.dump(skipped_files, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naumov_Python310 (torch_hf)",
   "language": "python",
   "name": "torch_hf_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
